{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5 summarization",
      "provenance": [],
      "mount_file_id": "15SCV3wf5laKYGjc08YVeNZRc_hy_p3Wg",
      "authorship_tag": "ABX9TyN4OM1P420Fv3qsKvWaaCmA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimmohraz/colab/blob/main/T5_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3RD1CMHsa7c"
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzZXu6ohtLj3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwCb8IUGtAK7"
      },
      "source": [
        "Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7UfC9b9te2J"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_vGD3PsthLp"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrrFMV44vu4D"
      },
      "source": [
        "# specifiy dataset and column names\n",
        "path = './data/news_summary.csv'\n",
        "summary_col = 'text'\n",
        "fulltext_col = 'ctext'\n",
        "\n",
        "config = {\n",
        "    'TRAIN_BATCH_SIZE': 4,     # input batch size for training (default: 64)\n",
        "    'VALID_BATCH_SIZE': 100,   # input batch size for testing (default: 1000)\n",
        "    'TRAIN_EPOCHS' : 3,        # number of epochs to train (default: 10)\n",
        "    'LEARNING_RATE' : 1e-4,    # learning rate (default: 0.01)\n",
        "    'SEED' : 42,               # random seed (default: 42)\n",
        "    'MAX_LEN' : 512,\n",
        "    'SUMMARY_LEN' : 150 \n",
        "    }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "932p8NhxeNw4"
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.summary)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fulltext = str(self.data.fulltext[index])\n",
        "        fulltext = ' '.join(fulltext.split())\n",
        "\n",
        "        summary = str(self.data.summary[index])\n",
        "        summary = ' '.join(summary.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([fulltext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([summary], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaPAR7TWmxoM"
      },
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we enumerate over the training loader and passed to the defined network \n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%50==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9TNdHlQ0CLz"
      },
      "source": [
        "def validate(tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5cc7sLztoqp"
      },
      "source": [
        "# Set random seeds and deterministic pytorch for reproducibility\n",
        "torch.manual_seed(config['SEED']) # pytorch random seed\n",
        "np.random.seed(config['SEED']) # numpy random seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# tokenzier for encoding the text\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4361KlRv0ivb",
        "outputId": "5616b6b9-5368-4a05-efbe-6bf7b29604c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Importing and Pre-Processing the domain data\n",
        "df = pd.read_csv(path ,encoding='latin-1')\n",
        "\n",
        "# Selecting the needed columns only and renaming to summary & fulltext.\n",
        "df = df[[summary_col, fulltext_col]]\n",
        "df = df.rename({summary_col: 'summary', fulltext_col: 'fulltext'}, axis='columns')\n",
        "# Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
        "df.fulltext = 'summarize: ' + df.fulltext\n",
        "# df = df.sample(frac=0.1)\n",
        "print(df.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             summary                                           fulltext\n",
            "0  The Administration of Union Territory Daman an...  summarize: The Daman and Diu administration on...\n",
            "1  Malaika Arora slammed an Instagram user who tr...  summarize: From her special numbers to TV?appe...\n",
            "2  The Indira Gandhi Institute of Medical Science...  summarize: The Indira Gandhi Institute of Medi...\n",
            "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  summarize: Lashkar-e-Taiba's Kashmir commander...\n",
            "4  Hotels in Maharashtra will train their staff t...  summarize: Hotels in Mumbai and other Indian c...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lVMz8qf04u-",
        "outputId": "e6015da9-cdff-42ab-cbb3-6a58e7a97624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Creation of Dataset and Dataloader\n",
        "# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state = config['SEED'])\n",
        "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "# Creating the Training and Validation dataset for further creation of Dataloader\n",
        "training_set = CustomDataset(train_dataset, tokenizer, config['MAX_LEN'], config['SUMMARY_LEN'])\n",
        "val_set = CustomDataset(val_dataset, tokenizer, config['MAX_LEN'], config['SUMMARY_LEN'])\n",
        "\n",
        "# Defining the parameters for creation of dataloaders\n",
        "train_params = {\n",
        "    'batch_size': config['TRAIN_BATCH_SIZE'],\n",
        "    'shuffle': True,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "val_params = {\n",
        "    'batch_size': config['VALID_BATCH_SIZE'],\n",
        "    'shuffle': False,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (4514, 2)\n",
            "TRAIN Dataset: (3611, 2)\n",
            "TEST Dataset: (903, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVl1jZ8i1Kk6",
        "outputId": "d2679dd6-f9c3-4ab2-f612-dc6837dee9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config['LEARNING_RATE'])\n",
        "\n",
        "# Training loop\n",
        "print('Initiating Fine-Tuning for the model on our dataset')\n",
        "\n",
        "for epoch in range(config['TRAIN_EPOCHS']):\n",
        "  train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "torch.save(model.state_dict(), './data/t5_summary_state_dict.pt')\n",
        "\n",
        "# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "# Saving the dataframe as predictions.csv\n",
        "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
        "final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "final_df.to_csv('./data/predictions.csv')\n",
        "print('Output Files generated for review')\n",
        "print(final_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating Fine-Tuning for the model on our dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1146: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  7.16775369644165\n",
            "Epoch: 0, Loss:  1.5576294660568237\n",
            "Epoch: 0, Loss:  2.176754951477051\n",
            "Epoch: 0, Loss:  1.914730191230774\n",
            "Epoch: 0, Loss:  2.052884340286255\n",
            "Epoch: 0, Loss:  1.3532397747039795\n",
            "Epoch: 0, Loss:  2.118084669113159\n",
            "Epoch: 0, Loss:  2.407341718673706\n",
            "Epoch: 0, Loss:  2.0403265953063965\n",
            "Epoch: 0, Loss:  1.359547734260559\n",
            "Epoch: 0, Loss:  2.276465654373169\n",
            "Epoch: 0, Loss:  1.4711371660232544\n",
            "Epoch: 0, Loss:  2.189418315887451\n",
            "Epoch: 0, Loss:  1.9185739755630493\n",
            "Epoch: 0, Loss:  2.204874277114868\n",
            "Epoch: 0, Loss:  1.62185537815094\n",
            "Epoch: 0, Loss:  2.595926523208618\n",
            "Epoch: 0, Loss:  1.535740613937378\n",
            "Epoch: 0, Loss:  1.7352827787399292\n",
            "Epoch: 1, Loss:  2.060634136199951\n",
            "Epoch: 1, Loss:  1.8844225406646729\n",
            "Epoch: 1, Loss:  1.2498791217803955\n",
            "Epoch: 1, Loss:  1.194539189338684\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}